{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM8HcwefoaeMIGgsVr3gkjm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JoanChege/CHATBOT1/blob/main/chatbottrial5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.DATA PREPARATION"
      ],
      "metadata": {
        "id": "nm1vI0lFlL-C"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vndSbVYVWrMX",
        "outputId": "a303eabb-aee3-4145-8833-1757c7fd7882"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "#first we need to import the required packages\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "import json\n",
        "import pickle\n",
        "\n",
        "import tensorflow as tf\n",
        "#from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "import numpy as np\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#we load the json file and extract the required data.\n",
        "#with open('/content/dataset.json') as file:\n",
        "   # data = json.load(file)\n",
        "\n",
        "words=[] #word list\n",
        "#empty list\n",
        "classes = []\n",
        "documents = []\n",
        "ignore_words = ['?', '!']\n",
        "\n",
        "data = open('/content/dataset.json').read()\n",
        "intents  = json.loads(data)"
      ],
      "metadata": {
        "id": "emZ15dasdf6y"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Iterates over each intent in the 'intents' list.\n",
        "For each intent, it iterates over the 'patterns' list associated with that intent.\n",
        "It tokenizes each pattern (sentence) using the nltk library's 'word_tokenize' function and converts the sentence into a list of words.\n",
        "It then extends the 'words' list with these newly tokenized words.\n",
        "It creates a tuple (wordList, intent['tag']) and appends it to the 'documents' list.\n",
        "It also checks if the tag of the current intent is already in the 'classes' list. If it's not, it appends the tag to the 'classes' list."
      ],
      "metadata": {
        "id": "Aph_zFB-LAmc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for intent in intents['intents']:\n",
        "  for pattern in intent['patterns']:\n",
        "    #we take each word and tokenize it\n",
        "    wordlist = nltk.word_tokenize(pattern)\n",
        "    words.extend(wordlist)\n",
        "\n",
        "    #add documents\n",
        "    documents.append((wordlist, intent['tag']))\n",
        "\n",
        "    #add to words list\n",
        "    for token in wordlist:\n",
        "      words.append(token)\n",
        "\n",
        "    #we add classes to our class list\n",
        "    if intent['tag'] not in classes:\n",
        "      classes.append(intent['tag'])"
      ],
      "metadata": {
        "id": "QXfO8-98I3qd"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "the first line takes each word in the words list and reduces the word to its base form by lemmatizing the word.\n",
        "the second line sorts the words list and removes and duplicates. the sorting is in ascending order.\n",
        "the next line sorts the classes list and removes any duplicate classes.\n",
        "the pickle dump saves the words and classes to a file that is the words.pkl and classes.pkl and wb is the parameter that indicates that the file is bwing opened for writing in binary mode.\n",
        "the above steps are used to convert the words into numerical form."
      ],
      "metadata": {
        "id": "azf8uT52L2mn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import collections\n",
        "\n",
        "# assuming you have a list of words called 'word_list'\n",
        "word_counts = collections.Counter(words)\n",
        "num_words = len(word_counts)\n",
        "\n",
        "print(f\"The number of unique words in the vocabulary is {num_words}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dk5-dPeacYU7",
        "outputId": "1b0b78c1-e335-4ed4-b7df-9e52f700596d"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The number of unique words in the vocabulary is 397\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "words = [lemmatizer.lemmatize(word) for word in words if word not in ignore_words]\n",
        "words = sorted(set(words))\n",
        "\n",
        "classes = sorted(set(classes))\n",
        "\n",
        "pickle.dump(words, open('words.pkl', 'wb'))\n",
        "pickle.dump(classes, open('classes.pkl', 'wb'))"
      ],
      "metadata": {
        "id": "z9xObYg0LoTy"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(data)"
      ],
      "metadata": {
        "id": "SjEIl7WUAvp3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#from nltk.tokenize import word_tokenize\n",
        "\n",
        "#intialize the training data\n",
        "training = []\n",
        "output_empty = [0] * len(classes)\n",
        "X_train = []\n",
        "Y_train = []\n",
        "\n",
        "for doc in documents:\n",
        "  #intialize bag of words and generate for each document\n",
        "  bow = []\n",
        "  #words = word_tokenize(doc[0])\n",
        "  #list of the tokenized words for the pattern\n",
        "  pattern_words = doc[0]\n",
        "  #lemmatize each word\n",
        "  pattern_words = [lemmatizer.lemmatize(word.lower()) for word in pattern_words]\n",
        "  #create the bag of words array with , if the word match found in current pattern\n",
        "  for wordlist in words:\n",
        "    bow.append(1) if wordlist in pattern_words else bow.append(0)\n",
        "\n",
        "  #output is 0 for each tag, and 1 for current tag\n",
        "  #this create the output vector\n",
        "  output_row = list(output_empty)\n",
        "  output_row[classes.index(doc[1])] = 1\n",
        "\n",
        "\n",
        "  #add the bag of words and output row to the training data\n",
        "  #num_samples = 100\n",
        "  #for i in range(num_samples):\n",
        "  #training.append([bow,  output_row])\n",
        "  #training = np.column_stack((training,[bow, output_row])) if len(training) else np.array([bow, output_row])\n",
        "  if len(training):\n",
        "    new_training = [[bow, output_row]]\n",
        "    for i in range(len(training[0])):\n",
        "      new_training.append(training[:, i])\n",
        "  else:\n",
        "    new_training = [[bow, output_row]]\n",
        "\n",
        "  training = np.column_stack(new_training)\n",
        "  #shuffle the features and turn into np.array\n",
        "  random.shuffle(new_training)\n",
        "  training = np.array(training, dtype=object) #could add ,dtype=object\n",
        "\n",
        "  # Convert the training data to a list from a numpy array\n",
        "  #training = training.tolist()\n",
        "X_train = list(training[:,0].reshape(-1,1))\n",
        "Y_train = list(training[:,1].reshape(-1,1))\n",
        "print(\"The training data has been created\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WFfRZKrbNh5F",
        "outputId": "91347e3e-e540-4e60-9197-c26d10c33d3e"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/numpy/lib/shape_base.py:652: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  arr = asanyarray(v)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The training data has been created\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(data)"
      ],
      "metadata": {
        "id": "y7JfQetxQbV7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Split the data into traing and testing sets"
      ],
      "metadata": {
        "id": "27iRkqhyW6WJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X_train, Y_train, test_size=0.1)"
      ],
      "metadata": {
        "id": "NrwNBdSN1e3I"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Modelling using LSTM- LONG SHORT TERM MEMORY\n",
        "ReLU- RECTIFIED LINEAR UNIT"
      ],
      "metadata": {
        "id": "dl_zdxNkVVVy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, LSTM, Bidirectional, Dropout, Dense"
      ],
      "metadata": {
        "id": "NpD1OJHtgZ94"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model is a Sequential model, meaning that it consists of a sequence of layers, where the output of each layer is the input to the next layer.\n",
        "\n",
        "---\n",
        "Embedding layer, which converts a sequence of words into a sequence of dense vectors of fixed size. In this case, the Embedding layer has an input dimension of 384, an output dimension of 100, and the input length is set to the length of the data sentences.\n",
        "\n",
        "\n",
        "---\n",
        "The Bidirectional layer is a wrapper around the LSTM layer that allows the model to process the input sequence in both directions, left-to-right and right-to-left. This can help capture the context of the data.\n",
        "\n",
        "---\n",
        "Following the Bidirectional layer, there are two Dropout layers, which are used to prevent overfitting by randomly setting a fraction of the input units to 0 at each update during training time. The Dropout rate is set to 0.5.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Next, the Dense layer is added, which is a fully connected layer that is used to learn the classification of the data. The output dimension of the Dense layer is set to 64, and the activation function is set to 'relu'.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "After the Dense layer, there is another Dropout layer, and finally, the Dense layer with a softmax activation function is added, which represents the final output of the model, where each element of the output vector represents the probability of the corresponding sentiment (positive or negative).\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "WOuKgMlNP9NH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.utils import to_categorical\n"
      ],
      "metadata": {
        "id": "8vwBHXiu9l4S"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_words = 397\n",
        "\n",
        "\n",
        "tokenizer = Tokenizer(num_words=num_words)\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "\n",
        "# Tokenizing sentences\n",
        "X_train_tokenized = tokenizer.texts_to_sequences(X_train)\n",
        "\n",
        "# Padding sequences to the same length\n",
        "X_train_padded = pad_sequences(X_train_tokenized, maxlen=len(X_train[0]), padding='post')\n",
        "\n",
        "# One-hot encoding the binary labels\n",
        "Y_train_one_hot = to_categorical(Y_train)\n"
      ],
      "metadata": {
        "id": "SOO-dI5690CD"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=num_words, output_dim=100, input_length=len(X_train[0])))\n",
        "model.add(Bidirectional(LSTM(100, return_sequences=True)))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(len(Y_train[0]), activation='softmax'))"
      ],
      "metadata": {
        "id": "d_fLjYImdbYn"
      },
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "1.   Import the necessary modules.\n",
        "2.Create a Sequential model.\n",
        "3.Add an Embedding layer to the model, which will convert the words into dense vectors of fixed size.\n",
        "4.Add a Bidirectional wrapper layer to the model. This layer is used to apply the LSTM in both forward and backward directions.\n",
        "5.Add an LSTM layer to the model. In this case, it is used as the underlying layer within the Bidirectional wrapper.\n",
        "6.Add a Dropout layer to the model to help prevent overfitting.\n",
        "7.Add a Dense layer to the model, which is used to learn the classification of the movie review.\n",
        "8.Add another Dropout layer to the model.\n",
        "9.Add a Dense layer with a softmax activation function to the model, which represents the final output of the model, where each element of the output vector represents the probability of the corresponding sentiment (positive or negative).\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "YNK08M_WgXBi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "30D1AJw4YmMH",
        "outputId": "f230c1de-2231-4f25-81e0-6d76cdbfcd98",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_6\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_5 (Embedding)     (None, 31, 100)           39700     \n",
            "                                                                 \n",
            " bidirectional_5 (Bidirecti  (None, 31, 200)           160800    \n",
            " onal)                                                           \n",
            "                                                                 \n",
            " dropout_10 (Dropout)        (None, 31, 200)           0         \n",
            "                                                                 \n",
            " dense_10 (Dense)            (None, 31, 64)            12864     \n",
            "                                                                 \n",
            " dropout_11 (Dropout)        (None, 31, 64)            0         \n",
            "                                                                 \n",
            " dense_11 (Dense)            (None, 31, 2)             130       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 213494 (833.96 KB)\n",
            "Trainable params: 213494 (833.96 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.   The first layer is an embedding layer. This layer is responsible for converting words into fixed-size vectors.\n",
        "2.  The second layer is a bidirectional LSTM layer. This layer takes the fixed-size vectors as input and processes them. The bidirectional part means that the LSTM layer learns the sequence from both the beginning and the end.\n",
        "3.   The third layer is a dropout layer. This layer helps to prevent overfitting by randomly dropping out a percentage of neurons during training.\n",
        "4.   The fourth layer is a dense layer. This layer combines the information from the LSTM layer to make a final prediction.\n",
        "5.  The fifth layer is another dropout layer.\n",
        "6.  The final layer is another dense layer. This layer has only one neuron and outputs the final probability that the given text is positive or negative.\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "To train the model, we pass it an input tensor and a target tensor. The input tensor contains the text, and the target tensor contains the labels. The model then learns to map the input text to the correct output label by adjusting its weights based on the loss function.\n"
      ],
      "metadata": {
        "id": "6DyuAJUlhvL6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.optimizers import SGD\n",
        "#use Stochastic Gradient Descent optimizer to train the LSTM model\n",
        "model.compile(optimizer = SGD(learning_rate=0.01,\n",
        "                            momentum=0.9, #help to improve the training speed and model stability\n",
        "                            nesterov=True)) #If this argument is set to True, the momentum term will be computed in the direction of the previous step, not in the direction of the current gradient. This modification provides better convergence and improved performance.\n"
      ],
      "metadata": {
        "id": "fUHZaT3HjI95"
      },
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.   Lose-loss function that the model aims to minimize during training. In this case, we are using 'categorical_crossentropy', which is a loss function commonly used for multi-class classification problems.\n",
        "2.   Optimizer- minimize the loss function during training\n",
        "1.   metrics- it specifies the evaluation metrics to be used during training and evaluation. In this case, we are using 'accuracy' as our evaluation metric.\n",
        "2.   List item\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "HrAAv4JKnTA7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss = 'categorical_crossentropy',\n",
        "              optimizer = 'sgd',\n",
        "              metrics = ['accuracy'])"
      ],
      "metadata": {
        "id": "Jceimws-TCLf"
      },
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Y_train_reshaped = tf.reshape(Y_train_one_hot, (len(Y_train_one_hot), -1, 2))"
      ],
      "metadata": {
        "id": "X4Ewx2smACOK"
      },
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hist = model.fit(X_train_padded, Y_train_reshaped, epochs=10, batch_size=5, verbose=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 737
        },
        "id": "viN-j4mzm_Bk",
        "outputId": "18599933-d9a7-4754-d9e2-b37cd87cb8ab"
      },
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-99-5aa6518ff2ec>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_padded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train_reshaped\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\u001b[0m in \u001b[0;36mtf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                     \u001b[0mretval_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_function\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m                 \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1377, in train_function  *\n        return step_function(self, iterator)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1360, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1349, in run_step  **\n        outputs = model.train_step(data)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1127, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1185, in compute_loss\n        return self.compiled_loss(\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/compile_utils.py\", line 277, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/losses.py\", line 143, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/losses.py\", line 270, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/losses.py\", line 2221, in categorical_crossentropy\n        return backend.categorical_crossentropy(\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/backend.py\", line 5575, in categorical_crossentropy\n        target.shape.assert_is_compatible_with(output.shape)\n\n    ValueError: Shapes (None, 2, 2) and (None, 31, 2) are incompatible\n"
          ]
        }
      ]
    }
  ]
}