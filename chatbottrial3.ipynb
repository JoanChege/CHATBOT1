{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPjGmq6c4/kblT9hC7N4kgM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JoanChege/CHATBOT1/blob/main/chatbottrial3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.DATA PREPARATION"
      ],
      "metadata": {
        "id": "nm1vI0lFlL-C"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vndSbVYVWrMX",
        "outputId": "3b74bdfd-fb00-467b-f376-3830cc4f1fe2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        }
      ],
      "source": [
        "#first we need to import the required packages\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "import json\n",
        "import pickle\n",
        "\n",
        "import numpy as np\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, LSTM, Activation, Dropout\n",
        "from keras.optimizers import SGD\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#we load the json file and extract the required data.\n",
        "#with open('/content/dataset.json') as file:\n",
        "   # data = json.load(file)\n",
        "\n",
        "words=[] #word list\n",
        "#empty list\n",
        "classes = []\n",
        "documents = []\n",
        "ignore_words = ['?', '!']\n",
        "\n",
        "data = open('/content/dataset.json').read()\n",
        "intents  = json.loads(data)"
      ],
      "metadata": {
        "id": "emZ15dasdf6y"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Iterates over each intent in the 'intents' list.\n",
        "For each intent, it iterates over the 'patterns' list associated with that intent.\n",
        "It tokenizes each pattern (sentence) using the nltk library's 'word_tokenize' function and converts the sentence into a list of words.\n",
        "It then extends the 'words' list with these newly tokenized words.\n",
        "It creates a tuple (wordList, intent['tag']) and appends it to the 'documents' list.\n",
        "It also checks if the tag of the current intent is already in the 'classes' list. If it's not, it appends the tag to the 'classes' list."
      ],
      "metadata": {
        "id": "Aph_zFB-LAmc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for intent in intents['intents']:\n",
        "  for pattern in intent['patterns']:\n",
        "    #we take each word and tokenize it\n",
        "    wordlist = nltk.word_tokenize(pattern)\n",
        "    words.extend(wordlist)\n",
        "\n",
        "    #add documents\n",
        "    documents.append((wordlist, intent['tag']))\n",
        "\n",
        "    #we add classes to our class list\n",
        "    if intent['tag'] not in classes:\n",
        "      classes.append(intent['tag'])"
      ],
      "metadata": {
        "id": "QXfO8-98I3qd"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "the first line takes each word in the words list and reduces the word to its base form by lemmatizing the word.\n",
        "the second line sorts the words list and removes and duplicates. the sorting is in ascending order.\n",
        "the next line sorts the classes list and removes any duplicate classes.\n",
        "the pickle dump saves the words and classes to a file that is the words.pkl and classes.pkl and wb is the parameter that indicates that the file is bwing opened for writing in binary mode.\n",
        "the above steps are used to convert the words into numerical form."
      ],
      "metadata": {
        "id": "azf8uT52L2mn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "words = [lemmatizer.lemmatize(word) for word in words if word not in ignore_words]\n",
        "words = sorted(set(words))\n",
        "\n",
        "classes = sorted(set(classes))\n",
        "\n",
        "pickle.dump(words, open('words.pkl', 'wb'))\n",
        "pickle.dump(classes, open('classes.pkl', 'wb'))"
      ],
      "metadata": {
        "id": "z9xObYg0LoTy"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "#intialize the training data\n",
        "training = []\n",
        "output_empty = [0] * len(classes)\n",
        "\n",
        "for doc in documents:\n",
        "  #intialize bag of words and generate for each document\n",
        "  bow = []\n",
        "         #words = word_tokenize(doc[0])\n",
        "  #list of the tokenized words for the pattern\n",
        "  pattern_words = doc[0]\n",
        "  #lemmatoze each word\n",
        "  pattern_words = [lemmatizer.lemmatize(word.lower()) for word in pattern_words]\n",
        "  #create the bag of words array with , if the word match found in current pattern\n",
        "  for wordlist in words:\n",
        "    bow.append(1) if wordlist in pattern_words else bow.append(0)\n",
        "\n",
        "  #output is 0 for each tag, and 1 for current tag\n",
        "  #this create the output vector\n",
        "  output_row = list(output_empty)\n",
        "  output_row[classes.index(doc[1])] = 1\n",
        "\n",
        "  #add the bag of words and output row to the training data\n",
        "  num_samples = 100\n",
        "  #for i in range(num_samples):\n",
        "  training.append(bow + output_row)\n",
        "\n",
        "\n",
        "  #shuffle the features and turn into np.array\n",
        "  random.shuffle(training)\n",
        "  training = np.array(training, dtype=object)\n",
        "\n",
        "  #create the train and test sets with X_patterns, and Y_intents\n",
        "  X_train = list(training[:,0])\n",
        "  Y_train = list(training[:,1])\n",
        "  print(\"The training data has been created\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 262
        },
        "id": "WFfRZKrbNh5F",
        "outputId": "0ef14d52-1617-4281-f2f5-4268e47f1ff7"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The training data has been created\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-60-ff382313aa44>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m   \u001b[0mnum_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m   \u001b[0;31m#for i in range(num_samples):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m   \u001b[0mtraining\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbow\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0moutput_row\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'append'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Split the data into traing and testing sets"
      ],
      "metadata": {
        "id": "27iRkqhyW6WJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X_train, Y_train, test_size=0.1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 338
        },
        "id": "2YsFRQBnW5z1",
        "outputId": "3b8dce89-f6df-4d85-cf1c-27b63409702d"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-61-7523e8216f2e>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36mtrain_test_split\u001b[0;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[1;32m   2560\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2561\u001b[0m     \u001b[0mn_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_num_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2562\u001b[0;31m     n_train, n_test = _validate_shuffle_split(\n\u001b[0m\u001b[1;32m   2563\u001b[0m         \u001b[0mn_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault_test_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.25\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2564\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36m_validate_shuffle_split\u001b[0;34m(n_samples, test_size, train_size, default_test_size)\u001b[0m\n\u001b[1;32m   2234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2235\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mn_train\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2236\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m   2237\u001b[0m             \u001b[0;34m\"With n_samples={}, test_size={} and train_size={}, the \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2238\u001b[0m             \u001b[0;34m\"resulting train set will be empty. Adjust any of the \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: With n_samples=1, test_size=0.1 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Modelling using LSTM"
      ],
      "metadata": {
        "id": "dl_zdxNkVVVy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First layer contains 128 neurons, second layer contains 64 neurons and 3rd output layer contains number of neurons equal to number of intents to predict output intent with softmax function"
      ],
      "metadata": {
        "id": "gXQFeJzUWPk2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#create model of 3 layers.\n",
        "model = Sequential()\n",
        "model.add(Dense(128, input_shape=(len(X_train[0]),),activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(len(Y_train[0]), activation='softmax'))\n"
      ],
      "metadata": {
        "id": "z-Qc5dXfVN-f"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}