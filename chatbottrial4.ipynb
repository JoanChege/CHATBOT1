{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMK6k+UAxxEHmmwIevEtFxu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JoanChege/CHATBOT1/blob/main/chatbottrial4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.DATA PREPARATION"
      ],
      "metadata": {
        "id": "nm1vI0lFlL-C"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vndSbVYVWrMX",
        "outputId": "2ac520e5-ed82-4a73-9c9a-5e083edb3cf0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        }
      ],
      "source": [
        "#first we need to import the required packages\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "import json\n",
        "import pickle\n",
        "\n",
        "import numpy as np\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, LSTM, Activation, Dropout\n",
        "from keras.optimizers import SGD\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#we load the json file and extract the required data.\n",
        "#with open('/content/dataset.json') as file:\n",
        "   # data = json.load(file)\n",
        "\n",
        "words=[] #word list\n",
        "#empty list\n",
        "classes = []\n",
        "documents = []\n",
        "ignore_words = ['?', '!']\n",
        "\n",
        "data = open('/content/dataset.json').read()\n",
        "intents  = json.loads(data)"
      ],
      "metadata": {
        "id": "emZ15dasdf6y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Iterates over each intent in the 'intents' list.\n",
        "For each intent, it iterates over the 'patterns' list associated with that intent.\n",
        "It tokenizes each pattern (sentence) using the nltk library's 'word_tokenize' function and converts the sentence into a list of words.\n",
        "It then extends the 'words' list with these newly tokenized words.\n",
        "It creates a tuple (wordList, intent['tag']) and appends it to the 'documents' list.\n",
        "It also checks if the tag of the current intent is already in the 'classes' list. If it's not, it appends the tag to the 'classes' list."
      ],
      "metadata": {
        "id": "Aph_zFB-LAmc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for intent in intents['intents']:\n",
        "  for pattern in intent['patterns']:\n",
        "    #we take each word and tokenize it\n",
        "    wordlist = nltk.word_tokenize(pattern)\n",
        "    words.extend(wordlist)\n",
        "\n",
        "    #add documents\n",
        "    documents.append((wordlist, intent['tag']))\n",
        "\n",
        "    #we add classes to our class list\n",
        "    if intent['tag'] not in classes:\n",
        "      classes.append(intent['tag'])"
      ],
      "metadata": {
        "id": "QXfO8-98I3qd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "the first line takes each word in the words list and reduces the word to its base form by lemmatizing the word.\n",
        "the second line sorts the words list and removes and duplicates. the sorting is in ascending order.\n",
        "the next line sorts the classes list and removes any duplicate classes.\n",
        "the pickle dump saves the words and classes to a file that is the words.pkl and classes.pkl and wb is the parameter that indicates that the file is bwing opened for writing in binary mode.\n",
        "the above steps are used to convert the words into numerical form."
      ],
      "metadata": {
        "id": "azf8uT52L2mn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "words = [lemmatizer.lemmatize(word) for word in words if word not in ignore_words]\n",
        "words = sorted(set(words))\n",
        "\n",
        "classes = sorted(set(classes))\n",
        "\n",
        "pickle.dump(words, open('words.pkl', 'wb'))\n",
        "pickle.dump(classes, open('classes.pkl', 'wb'))"
      ],
      "metadata": {
        "id": "z9xObYg0LoTy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(data)"
      ],
      "metadata": {
        "id": "SjEIl7WUAvp3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "#intialize the training data\n",
        "training = []\n",
        "output_empty = [0] * len(classes)\n",
        "X_train = []\n",
        "Y_train = []\n",
        "\n",
        "for doc in documents:\n",
        "  #intialize bag of words and generate for each document\n",
        "  bow = []\n",
        "         #words = word_tokenize(doc[0])\n",
        "  #list of the tokenized words for the pattern\n",
        "  pattern_words = doc[0]\n",
        "  #lemmaioze each word\n",
        "  pattern_words = [lemmatizer.lemmatize(word.lower()) for word in pattern_words]\n",
        "  #create the bag of words array with , if the word match found in current pattern\n",
        "  for wordlist in words:\n",
        "    bow.append(1) if wordlist in pattern_words else bow.append(0)\n",
        "\n",
        "  #output is 0 for each tag, and 1 for current tag\n",
        "  #this create the output vector\n",
        "  output_row = list(output_empty)\n",
        "  output_row[classes.index(doc[1])] = 1\n",
        "\n",
        "\n",
        "  #add the bag of words and output row to the training data\n",
        "  #num_samples = 100\n",
        "  #for i in range(num_samples):\n",
        "  #training.append([bow,  output_row])\n",
        "  training = np.column_stack((training,[bow, output_row])) if len(training) else np.array([bow, output_row])\n",
        "\n",
        "\n",
        "  #shuffle the features and turn into np.array\n",
        "  random.shuffle(training)\n",
        "  #training = np.array(training) #could add ,dtype=object\n",
        "\n",
        "  # Convert the training data to a list from a numpy array\n",
        "  #training = training.tolist()\n",
        "X_train = list(training[:,0].reshape(-1,1))\n",
        "Y_train = list(training[:,1].reshape(-1,1))\n",
        "print(\"The training data has been created\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WFfRZKrbNh5F",
        "outputId": "4f75d851-2a6f-4ae1-c249-66c7b295e5cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The training data has been created\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-26-0dc80b302474>:31: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  training = np.column_stack((training,[bow, output_row])) if len(training) else np.array([bow, output_row])\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/lib/shape_base.py:652: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  arr = asanyarray(v)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(data)"
      ],
      "metadata": {
        "id": "y7JfQetxQbV7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Split the data into traing and testing sets"
      ],
      "metadata": {
        "id": "27iRkqhyW6WJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X_train, Y_train, test_size=0.1)"
      ],
      "metadata": {
        "id": "2YsFRQBnW5z1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Modelling using LSTM"
      ],
      "metadata": {
        "id": "dl_zdxNkVVVy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First layer contains 128 neurons, second layer contains 64 neurons and 3rd output layer contains number of neurons equal to number of intents to predict output intent with softmax function"
      ],
      "metadata": {
        "id": "gXQFeJzUWPk2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential, load_model\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, GRU"
      ],
      "metadata": {
        "id": "NpD1OJHtgZ94"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#create model of 3 layers.\n",
        "#model = Sequential()\n",
        "#model.add(Dense(128, input_shape=(len(X_train[0]),),activation='relu'))\n",
        "#model.add(Dropout(0.5))\n",
        "#model.add(Dense(64, activation='relu'))\n",
        "#model.add(Dropout(0.5))\n",
        "#model.add(Dense(len(Y_train[0]), activation='softmax'))\n"
      ],
      "metadata": {
        "id": "z-Qc5dXfVN-f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#embedding - Bi-LSTM - Dense - Dense\n",
        "model = Sequential([\n",
        "                 tf.keras.layers.Embedding(10000, 64),\n",
        "                 tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),\n",
        "                 tf.keras.layers.Dense(64, activation = 'relu'),\n",
        "                 tf.keras.layers.Dense(7, activation = 'softmax' )\n",
        "])\n"
      ],
      "metadata": {
        "id": "pQ4w2aqDfuzZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "30D1AJw4YmMH",
        "outputId": "57e12c66-9222-4642-ff11-a329cd7b99b6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, None, 64)          640000    \n",
            "                                                                 \n",
            " bidirectional (Bidirection  (None, 128)               66048     \n",
            " al)                                                             \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 64)                8256      \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 7)                 455       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 714759 (2.73 MB)\n",
            "Trainable params: 714759 (2.73 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    }
  ]
}